{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d619851",
   "metadata": {},
   "source": [
    "# 1. Import libraries and load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbdb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(r'D:\\Topic_13_Retail_Store_Sales_Time_Series\\Topic_13_Retail_Store_Sales_Time_Series\\data\\raw\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952e7c5",
   "metadata": {},
   "source": [
    "# 2. Display basic information about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462df26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Rows of Data Frame:\n",
      "    id        date  store_nbr      family  sales  onpromotion\n",
      "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1       BOOKS    0.0            0\n",
      "Data Frame Shape:\n",
      " (3000888, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   id           int64  \n",
      " 1   date         object \n",
      " 2   store_nbr    int64  \n",
      " 3   family       object \n",
      " 4   sales        float64\n",
      " 5   onpromotion  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 137.4+ MB\n",
      "Data Frame Info:\n",
      " None\n",
      "Data Frame Statistics:\n",
      "                  id     store_nbr         sales   onpromotion\n",
      "count  3.000888e+06  3.000888e+06  3.000888e+06  3.000888e+06\n",
      "mean   1.500444e+06  2.750000e+01  3.577757e+02  2.602770e+00\n",
      "std    8.662819e+05  1.558579e+01  1.101998e+03  1.221888e+01\n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    7.502218e+05  1.400000e+01  0.000000e+00  0.000000e+00\n",
      "50%    1.500444e+06  2.750000e+01  1.100000e+01  0.000000e+00\n",
      "75%    2.250665e+06  4.100000e+01  1.958473e+02  0.000000e+00\n",
      "max    3.000887e+06  5.400000e+01  1.247170e+05  7.410000e+02\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 Rows of Data Frame:\\n\", df_train.head(5))\n",
    "print(\"Data Frame Shape:\\n\", df_train.shape)\n",
    "print(\"Data Frame Info:\\n\", df_train.info())\n",
    "print(\"Data Frame Statistics:\\n\", df_train.describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c26d69",
   "metadata": {},
   "source": [
    "# 3. Missing Values Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a985ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "              Missing Values  Percentage\n",
      "id                        0         0.0\n",
      "date                      0         0.0\n",
      "store_nbr                 0         0.0\n",
      "family                    0         0.0\n",
      "sales                     0         0.0\n",
      "onpromotion               0         0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing values\n",
    "missing_counts = df_train.isnull().sum()\n",
    "missing_percentage = missing_counts / len(df_train) * 100\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_df_train = pd.DataFrame({\n",
    "    'Missing Values': missing_counts,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\\n\", missing_df_train)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_df_train = missing_df_train[missing_df_train['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187bea4",
   "metadata": {},
   "source": [
    "# 4. Detailed Analyze and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53839ca7",
   "metadata": {},
   "source": [
    "### Analyze 'sales' column and calculate detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4106b",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze 'sales' column and calculate detailed statistics\n# Total sales and basic stats\ntotal_sales = df_train['sales'].sum()\navg_sales = df_train['sales'].mean()\nmedian_sales = df_train['sales'].median()\nstd_sales = df_train['sales'].std()\nzero_sales_count = (df_train['sales'] == 0.0).sum()\npct_zero_sales = zero_sales_count / len(df_train) * 100\nprint(f\"Total sales: ${total_sales:,.2f}\")\nprint(f\"Average sales: ${avg_sales:.2f}\")\nprint(f\"Median sales: ${median_sales:.2f}\")\nprint(f\"Std dev sales: ${std_sales:.2f}\")\nprint(f\"Zero sales count: {zero_sales_count:,} ({pct_zero_sales:.2f}%)\")\n\n# Sales by store and family (sums and means)\nsales_by_store = df_train.groupby('store_nbr')['sales'].sum().sort_values(ascending=False)\nsales_by_family = df_train.groupby('family')['sales'].sum().sort_values(ascending=False)\nsales_by_store_mean = df_train.groupby('store_nbr')['sales'].mean().sort_values(ascending=False)\nsales_by_family_mean = df_train.groupby('family')['sales'].mean().sort_values(ascending=False)\n\nprint('\\nTop 10 stores by sales:\\n', sales_by_store.head(10))\nprint('\\nTop 10 families by sales:\\n', sales_by_family.head(10))\n\n# Daily sales (time series) - ensure 'date' is datetime\ndf_train['date'] = pd.to_datetime(df_train['date'])\ndaily_sales = df_train.groupby('date')['sales'].sum()\nprint('\\nDaily sales sample:\\n', daily_sales.head())\n\n# Create a summary DataFrame\nsales_summary = pd.DataFrame({\n    'total_sales': [total_sales],\n    'avg_sales': [avg_sales],\n    'median_sales': [median_sales],\n    'std_sales': [std_sales],\n    'zero_sales_count': [zero_sales_count],\n    'pct_zero_sales': [pct_zero_sales]\n})\nprint('\\nSales summary:\\n', sales_summary)"
  },
  {
   "cell_type": "markdown",
   "id": "zeifhlub6vm",
   "source": "### Sales Distribution Analysis\n\nAnalyzing the shape and characteristics of the sales distribution to understand skewness, outliers, and spread.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9n72m7xm1fe",
   "source": "# Sales Distribution Characteristics\nfrom scipy import stats\n\n# Calculate distribution metrics\nmin_sales = df_train['sales'].min()\nmax_sales = df_train['sales'].max()\nq25 = df_train['sales'].quantile(0.25)\nq75 = df_train['sales'].quantile(0.75)\nq99 = df_train['sales'].quantile(0.99)\n\n# Skewness and Kurtosis\nskewness = stats.skew(df_train['sales'])\nkurtosis_val = stats.kurtosis(df_train['sales'])\n\n# Coefficient of Variation (std/mean)\ncv = (std_sales / avg_sales) * 100\n\n# Gap between mean and median\nmean_median_gap = ((avg_sales - median_sales) / median_sales) * 100\n\n# Create distribution summary table\ndistribution_stats = pd.DataFrame({\n    'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', '25th Percentile', '75th Percentile', '99th Percentile', \n               'Skewness', 'Kurtosis', 'Coefficient of Variation (%)', 'Mean-Median Gap (%)'],\n    'Value': [f'${avg_sales:.2f}', f'${median_sales:.2f}', f'${std_sales:.2f}', f'${min_sales:.2f}', \n              f'${max_sales:,.2f}', f'${q25:.2f}', f'${q75:.2f}', f'${q99:,.2f}',\n              f'{skewness:.2f}', f'{kurtosis_val:.2f}', f'{cv:.1f}%', f'{mean_median_gap:.1f}%']\n})\n\nprint(\"Sales Distribution Statistics:\")\nprint(distribution_stats.to_string(index=False))\n\nprint(f\"\\nðŸ“Š Interpretation:\")\nprint(f\"- Mean is {mean_median_gap:.1f}% higher than median, indicating RIGHT-SKEWED distribution\")\nprint(f\"- High CV ({cv:.1f}%) indicates EXTREME VARIABILITY in sales\")\nprint(f\"- Skewness of {skewness:.2f} confirms heavy right tail (large sales are rare but impactful)\")\nprint(f\"- Maximum sales (${max_sales:,.0f}) is {max_sales/avg_sales:.0f}x the mean\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ie0c3azpcw",
   "source": "### Store Performance Concentration Analysis\n\nAnalyzing how sales are distributed across stores to identify concentration and top performers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ot633bkwx8",
   "source": "# Store-Level Concentration Metrics\n\n# Calculate cumulative percentages\nstore_sales_sorted = sales_by_store.sort_values(ascending=False)\nstore_cumsum = store_sales_sorted.cumsum()\nstore_cumsum_pct = (store_cumsum / total_sales) * 100\n\n# Create top 10 stores with concentration metrics\ntop_10_stores = pd.DataFrame({\n    'Store #': store_sales_sorted.head(10).index,\n    'Total Sales': [f'${x:,.0f}' for x in store_sales_sorted.head(10).values],\n    '% of Total': [f'{(x/total_sales)*100:.1f}%' for x in store_sales_sorted.head(10).values],\n    'Cumulative %': [f'{store_cumsum_pct.iloc[i]:.1f}%' for i in range(10)]\n})\n\nprint(\"Top 10 Stores by Sales (with Concentration Metrics):\")\nprint(top_10_stores.to_string(index=False))\n\n# Calculate concentration statistics\ntop_4_stores_pct = (store_sales_sorted.head(4).sum() / total_sales) * 100\ntop_10_stores_pct = (store_sales_sorted.head(10).sum() / total_sales) * 100\ntotal_stores = df_train['store_nbr'].nunique()\n\n# Store heterogeneity: max vs min\nmax_store_sales = store_sales_sorted.iloc[0]\nmin_store_sales = store_sales_sorted.iloc[-1]\nstore_variance_ratio = max_store_sales / min_store_sales\n\nprint(f\"\\nðŸ“ˆ Store Concentration Insights:\")\nprint(f\"- Top 4 stores (7.4% of locations) generate {top_4_stores_pct:.1f}% of revenue\")\nprint(f\"- Top 10 stores ({10/total_stores*100:.1f}% of locations) generate {top_10_stores_pct:.1f}% of revenue\")\nprint(f\"- Highest performing store (#{store_sales_sorted.index[0]}) generates {store_variance_ratio:.1f}x more than lowest\")\nprint(f\"- This indicates EXTREME HETEROGENEITY across store locations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "k52qb7su79",
   "source": "### Product Family Concentration Analysis\n\nAnalyzing which product families drive the most revenue and their contribution to total sales.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6v5yopx5rro",
   "source": "# Product Family Concentration Metrics\n\n# Calculate cumulative percentages for families\nfamily_sales_sorted = sales_by_family.sort_values(ascending=False)\nfamily_cumsum = family_sales_sorted.cumsum()\nfamily_cumsum_pct = (family_cumsum / total_sales) * 100\n\n# Create top 10 families with concentration metrics\ntop_10_families = pd.DataFrame({\n    'Product Family': family_sales_sorted.head(10).index,\n    'Total Sales': [f'${x:,.0f}' for x in family_sales_sorted.head(10).values],\n    '% of Total': [f'{(x/total_sales)*100:.1f}%' for x in family_sales_sorted.head(10).values],\n    'Cumulative %': [f'{family_cumsum_pct.iloc[i]:.1f}%' for i in range(10)]\n})\n\nprint(\"Top 10 Product Families by Sales (with Concentration Metrics):\")\nprint(top_10_families.to_string(index=False))\n\n# Calculate concentration statistics\ntop_3_families_pct = (family_sales_sorted.head(3).sum() / total_sales) * 100\ntop_5_families_pct = (family_sales_sorted.head(5).sum() / total_sales) * 100\ntop_10_families_pct = (family_sales_sorted.head(10).sum() / total_sales) * 100\ntotal_families = df_train['family'].nunique()\n\nprint(f\"\\nðŸ“Š Product Family Concentration Insights:\")\nprint(f\"- Top 3 families (GROCERY I, BEVERAGES, PRODUCE) account for {top_3_families_pct:.1f}% of total revenue\")\nprint(f\"- Top 5 families ({5/total_families*100:.1f}% of categories) generate {top_5_families_pct:.1f}% of revenue\")\nprint(f\"- Top 10 families ({10/total_families*100:.1f}% of categories) generate {top_10_families_pct:.1f}% of revenue\")\nprint(f\"- This shows HEAVY CONCENTRATION in daily necessities categories\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2a6679bc",
   "metadata": {},
   "source": [
    "### Analyze 'onpromotion' column and calculate detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501eb23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'onpromotion' occurrences: 7810622\n",
      "Percentage of rows with promotion: 260.28%\n",
      "Zero 'onpromotion' count: 2389559 (79.63%)\n",
      "\n",
      "Top 10 stores by promotion count:\n",
      " store_nbr\n",
      "53    204016\n",
      "47    192725\n",
      "44    192449\n",
      "45    191503\n",
      "46    190697\n",
      "48    185566\n",
      "49    184736\n",
      "9     177356\n",
      "3     177075\n",
      "50    174115\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Top 10 families by promotion count:\n",
      " family\n",
      "GROCERY I        1914801\n",
      "PRODUCE          1117921\n",
      "BEVERAGES         906958\n",
      "DAIRY             728707\n",
      "CLEANING          661157\n",
      "DELI              583316\n",
      "BREAD/BAKERY      331289\n",
      "MEATS             304028\n",
      "PERSONAL CARE     246928\n",
      "POULTRY           226421\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Daily promotion sample:\n",
      " date\n",
      "2013-01-01    0\n",
      "2013-01-02    0\n",
      "2013-01-03    0\n",
      "2013-01-04    0\n",
      "2013-01-05    0\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Promotion summary:\n",
      "    total_onpromotion  pct_onpromotion  unique_promo_days\n",
      "0            7810622       260.277025               1230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Total promotion occurrences and percentage\n",
    "total_onpromo = df_train['onpromotion'].sum()\n",
    "pct_onpromo = total_onpromo / len(df_train) * 100\n",
    "zero_onpromo_count = (df_train['onpromotion'] == 0).sum()\n",
    "pct_zero_onpromo = zero_onpromo_count / len(df_train) * 100\n",
    "print(f\"Total 'onpromotion' occurrences: {total_onpromo}\")\n",
    "print(f\"Percentage of rows with promotion: {pct_onpromo:.2f}%\")\n",
    "print(f\"Zero 'onpromotion' count: {zero_onpromo_count} ({pct_zero_onpromo:.2f}%)\")\n",
    "\n",
    "# Promotions by store and family (counts and percent)\n",
    "onpromo_by_store = df_train.groupby('store_nbr')['onpromotion'].sum().sort_values(ascending=False)\n",
    "onpromo_by_family = df_train.groupby('family')['onpromotion'].sum().sort_values(ascending=False)\n",
    "onpromo_by_store_pct = (df_train.groupby('store_nbr')['onpromotion'].mean()*100).sort_values(ascending=False)\n",
    "onpromo_by_family_pct = (df_train.groupby('family')['onpromotion'].mean()*100).sort_values(ascending=False)\n",
    "\n",
    "print('\\nTop 10 stores by promotion count:\\n', onpromo_by_store.head(10))\n",
    "print('\\nTop 10 families by promotion count:\\n', onpromo_by_family.head(10))\n",
    "\n",
    "# Daily promotion counts (time series) - ensure 'date' is datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "daily_onpromo = df_train.groupby('date')['onpromotion'].sum()\n",
    "print('\\nDaily promotion sample:\\n', daily_onpromo.head())\n",
    "\n",
    "# Create a summary DataFrame\n",
    "promotion_summary = pd.DataFrame({\n",
    "    'total_onpromotion': [total_onpromo],\n",
    "    'pct_onpromotion': [pct_onpromo],\n",
    "    'unique_promo_days': [daily_onpromo[daily_onpromo>0].shape[0]]\n",
    "})\n",
    "print('\\nPromotion summary:\\n', promotion_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zozgvaxxkjj",
   "source": "### Promotional Strategy Analysis\n\nAnalyzing how promotions are distributed across stores and product families.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qc97ci7k8h",
   "source": "# Promotional Concentration Analysis\n\n# Top stores by promotional activity\npromo_sorted_stores = onpromo_by_store.sort_values(ascending=False)\npromo_cumsum_stores = promo_sorted_stores.cumsum()\npromo_cumsum_pct_stores = (promo_cumsum_stores / total_onpromo) * 100\n\ntop_5_promo_stores = pd.DataFrame({\n    'Store #': promo_sorted_stores.head(5).index,\n    'Total Promo Items': [f'{x:,}' for x in promo_sorted_stores.head(5).values],\n    '% of All Promos': [f'{(x/total_onpromo)*100:.1f}%' for x in promo_sorted_stores.head(5).values],\n    'Cumulative %': [f'{promo_cumsum_pct_stores.iloc[i]:.1f}%' for i in range(5)]\n})\n\nprint(\"Top 5 Stores by Promotional Activity:\")\nprint(top_5_promo_stores.to_string(index=False))\n\n# Top families by promotional activity  \npromo_sorted_families = onpromo_by_family.sort_values(ascending=False)\npromo_cumsum_families = promo_sorted_families.cumsum()\npromo_cumsum_pct_families = (promo_cumsum_families / total_onpromo) * 100\n\ntop_5_promo_families = pd.DataFrame({\n    'Product Family': promo_sorted_families.head(5).index,\n    'Total Promo Items': [f'{x:,}' for x in promo_sorted_families.head(5).values],\n    '% of All Promos': [f'{(x/total_onpromo)*100:.1f}%' for x in promo_sorted_families.head(5).values],\n    'Cumulative %': [f'{promo_cumsum_pct_families.iloc[i]:.1f}%' for i in range(5)]\n})\n\nprint(\"\\nTop 5 Product Families by Promotional Activity:\")\nprint(top_5_promo_families.to_string(index=False))\n\n# Promotional insights\ntop_5_promo_families_pct = (promo_sorted_families.head(5).sum() / total_onpromo) * 100\ndays_with_promos = (daily_onpromo > 0).sum()\ntotal_days = daily_onpromo.shape[0]\ndays_with_promos_pct = (days_with_promos / total_days) * 100\n\nprint(f\"\\nðŸŽ¯ Promotional Strategy Insights:\")\nprint(f\"- {pct_zero_onpromo:.1f}% of records have NO promotions, while {100-pct_zero_onpromo:.1f}% have at least one\")\nprint(f\"- Promotions active on {days_with_promos} out of {total_days} days ({days_with_promos_pct:.1f}%)\")\nprint(f\"- Top 5 families receive {top_5_promo_families_pct:.1f}% of all promotional activity\")\nprint(f\"- Promotions are CATEGORY-SPECIFIC, targeting high-revenue families\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a31c08ae",
   "metadata": {},
   "source": [
    "# 5. Handling Missong Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0dab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}