{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d619851",
   "metadata": {},
   "source": [
    "# 1. Import libraries and load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbdb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(r'D:\\Topic_13_Project\\Topic_13_Retail_Store_Sales_Time_Series\\data\\raw\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952e7c5",
   "metadata": {},
   "source": [
    "# 2. Display basic information about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462df26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Rows of Data Frame:\n",
      "    id        date  store_nbr      family  sales  onpromotion\n",
      "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1       BOOKS    0.0            0\n",
      "Data Frame Shape:\n",
      " (3000888, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   id           int64  \n",
      " 1   date         object \n",
      " 2   store_nbr    int64  \n",
      " 3   family       object \n",
      " 4   sales        float64\n",
      " 5   onpromotion  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 137.4+ MB\n",
      "Data Frame Info:\n",
      " None\n",
      "Data Frame Statistics:\n",
      "                  id     store_nbr         sales   onpromotion\n",
      "count  3.000888e+06  3.000888e+06  3.000888e+06  3.000888e+06\n",
      "mean   1.500444e+06  2.750000e+01  3.577757e+02  2.602770e+00\n",
      "std    8.662819e+05  1.558579e+01  1.101998e+03  1.221888e+01\n",
      "min    0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    7.502218e+05  1.400000e+01  0.000000e+00  0.000000e+00\n",
      "50%    1.500444e+06  2.750000e+01  1.100000e+01  0.000000e+00\n",
      "75%    2.250665e+06  4.100000e+01  1.958473e+02  0.000000e+00\n",
      "max    3.000887e+06  5.400000e+01  1.247170e+05  7.410000e+02\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 Rows of Data Frame:\\n\", df_train.head(5))\n",
    "print(\"Data Frame Shape:\\n\", df_train.shape)\n",
    "print(\"Data Frame Info:\\n\", df_train.info())\n",
    "print(\"Data Frame Statistics:\\n\", df_train.describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c26d69",
   "metadata": {},
   "source": [
    "# 3. Missing Values Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a985ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "              Missing Values  Percentage\n",
      "id                        0         0.0\n",
      "date                      0         0.0\n",
      "store_nbr                 0         0.0\n",
      "family                    0         0.0\n",
      "sales                     0         0.0\n",
      "onpromotion               0         0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing values\n",
    "missing_counts = df_train.isnull().sum()\n",
    "missing_percentage = missing_counts / len(df_train) * 100\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_df_train = pd.DataFrame({\n",
    "    'Missing Values': missing_counts,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\\n\", missing_df_train)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_df_train = missing_df_train[missing_df_train['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187bea4",
   "metadata": {},
   "source": [
    "# 4. Detailed Analyze and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53839ca7",
   "metadata": {},
   "source": [
    "### Analyze 'sales' column and calculate detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fa4106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales: $1,073,644,952.20\n",
      "Average sales: $357.78\n",
      "Median sales: $11.00\n",
      "Std dev sales: $1102.00\n",
      "Zero sales count: 939,130 (31.30%)\n",
      "\n",
      "Top 10 stores by sales:\n",
      " store_nbr\n",
      "44    6.208755e+07\n",
      "45    5.449801e+07\n",
      "47    5.094831e+07\n",
      "3     5.048191e+07\n",
      "49    4.342010e+07\n",
      "46    4.189606e+07\n",
      "48    3.593313e+07\n",
      "51    3.291149e+07\n",
      "8     3.049429e+07\n",
      "50    2.865302e+07\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Top 10 families by sales:\n",
      " family\n",
      "GROCERY I        3.434627e+08\n",
      "BEVERAGES        2.169545e+08\n",
      "PRODUCE          1.227047e+08\n",
      "CLEANING         9.752129e+07\n",
      "DAIRY            6.448771e+07\n",
      "BREAD/BAKERY     4.213395e+07\n",
      "POULTRY          3.187600e+07\n",
      "MEATS            3.108647e+07\n",
      "PERSONAL CARE    2.459205e+07\n",
      "DELI             2.411032e+07\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Daily sales sample:\n",
      " date\n",
      "2013-01-01      2511.618999\n",
      "2013-01-02    496092.417944\n",
      "2013-01-03    361461.231124\n",
      "2013-01-04    354459.677093\n",
      "2013-01-05    477350.121229\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Sales summary:\n",
      "     total_sales   avg_sales  median_sales    std_sales  zero_sales_count  \\\n",
      "0  1.073645e+09  357.775749          11.0  1101.997721            939130   \n",
      "\n",
      "   pct_zero_sales  \n",
      "0        31.29507  \n"
     ]
    }
   ],
   "source": [
    "# Analyze 'sales' column and calculate detailed statistics\n",
    "# Total sales and basic stats\n",
    "total_sales = df_train['sales'].sum()\n",
    "avg_sales = df_train['sales'].mean()\n",
    "median_sales = df_train['sales'].median()\n",
    "std_sales = df_train['sales'].std()\n",
    "zero_sales_count = (df_train['sales'] == 0.0).sum()\n",
    "pct_zero_sales = zero_sales_count / len(df_train) * 100\n",
    "print(f\"Total sales: ${total_sales:,.2f}\")\n",
    "print(f\"Average sales: ${avg_sales:.2f}\")\n",
    "print(f\"Median sales: ${median_sales:.2f}\")\n",
    "print(f\"Std dev sales: ${std_sales:.2f}\")\n",
    "print(f\"Zero sales count: {zero_sales_count:,} ({pct_zero_sales:.2f}%)\")\n",
    "\n",
    "# Sales by store and family (sums and means)\n",
    "sales_by_store = df_train.groupby('store_nbr')['sales'].sum().sort_values(ascending=False)\n",
    "sales_by_family = df_train.groupby('family')['sales'].sum().sort_values(ascending=False)\n",
    "sales_by_store_mean = df_train.groupby('store_nbr')['sales'].mean().sort_values(ascending=False)\n",
    "sales_by_family_mean = df_train.groupby('family')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "print('\\nTop 10 stores by sales:\\n', sales_by_store.head(10))\n",
    "print('\\nTop 10 families by sales:\\n', sales_by_family.head(10))\n",
    "\n",
    "# Daily sales (time series) - ensure 'date' is datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "daily_sales = df_train.groupby('date')['sales'].sum()\n",
    "print('\\nDaily sales sample:\\n', daily_sales.head())\n",
    "\n",
    "# Create a summary DataFrame\n",
    "sales_summary = pd.DataFrame({\n",
    "    'total_sales': [total_sales],\n",
    "    'avg_sales': [avg_sales],\n",
    "    'median_sales': [median_sales],\n",
    "    'std_sales': [std_sales],\n",
    "    'zero_sales_count': [zero_sales_count],\n",
    "    'pct_zero_sales': [pct_zero_sales]\n",
    "})\n",
    "print('\\nSales summary:\\n', sales_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeifhlub6vm",
   "metadata": {},
   "source": [
    "### Sales Distribution Analysis\n",
    "\n",
    "Analyzing the shape and characteristics of the sales distribution to understand skewness, outliers, and spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9n72m7xm1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Distribution Statistics:\n",
      "                      Metric       Value\n",
      "                        Mean     $357.78\n",
      "                      Median      $11.00\n",
      "                     Std Dev    $1102.00\n",
      "                         Min       $0.00\n",
      "                         Max $124,717.00\n",
      "             25th Percentile       $0.00\n",
      "             75th Percentile     $195.85\n",
      "             99th Percentile   $5,507.00\n",
      "                    Skewness        7.36\n",
      "                    Kurtosis      154.56\n",
      "Coefficient of Variation (%)      308.0%\n",
      "         Mean-Median Gap (%)     3152.5%\n",
      "\n",
      " Interpretation:\n",
      "- Mean is 3152.5% higher than median, indicating RIGHT-SKEWED distribution\n",
      "- High CV (308.0%) indicates EXTREME VARIABILITY in sales\n",
      "- Skewness of 7.36 confirms heavy right tail (large sales are rare but impactful)\n",
      "- Maximum sales ($124,717) is 349x the mean\n"
     ]
    }
   ],
   "source": [
    "# Sales Distribution Characteristics\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate distribution metrics\n",
    "min_sales = df_train['sales'].min()\n",
    "max_sales = df_train['sales'].max()\n",
    "q25 = df_train['sales'].quantile(0.25)\n",
    "q75 = df_train['sales'].quantile(0.75)\n",
    "q99 = df_train['sales'].quantile(0.99)\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "skewness = stats.skew(df_train['sales'])\n",
    "kurtosis_val = stats.kurtosis(df_train['sales'])\n",
    "\n",
    "# Coefficient of Variation (std/mean)\n",
    "cv = (std_sales / avg_sales) * 100\n",
    "\n",
    "# Gap between mean and median\n",
    "mean_median_gap = ((avg_sales - median_sales) / median_sales) * 100\n",
    "\n",
    "# Create distribution summary table\n",
    "distribution_stats = pd.DataFrame({\n",
    "    'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', '25th Percentile', '75th Percentile', '99th Percentile', \n",
    "               'Skewness', 'Kurtosis', 'Coefficient of Variation (%)', 'Mean-Median Gap (%)'],\n",
    "    'Value': [f'${avg_sales:.2f}', f'${median_sales:.2f}', f'${std_sales:.2f}', f'${min_sales:.2f}', \n",
    "              f'${max_sales:,.2f}', f'${q25:.2f}', f'${q75:.2f}', f'${q99:,.2f}',\n",
    "              f'{skewness:.2f}', f'{kurtosis_val:.2f}', f'{cv:.1f}%', f'{mean_median_gap:.1f}%']\n",
    "})\n",
    "\n",
    "print(\"Sales Distribution Statistics:\")\n",
    "print(distribution_stats.to_string(index=False))\n",
    "\n",
    "print(f\"\\n Interpretation:\")\n",
    "print(f\"- Mean is {mean_median_gap:.1f}% higher than median, indicating RIGHT-SKEWED distribution\")\n",
    "print(f\"- High CV ({cv:.1f}%) indicates EXTREME VARIABILITY in sales\")\n",
    "print(f\"- Skewness of {skewness:.2f} confirms heavy right tail (large sales are rare but impactful)\")\n",
    "print(f\"- Maximum sales (${max_sales:,.0f}) is {max_sales/avg_sales:.0f}x the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ie0c3azpcw",
   "metadata": {},
   "source": [
    "### Store Performance Concentration Analysis\n",
    "\n",
    "Analyzing how sales are distributed across stores to identify concentration and top performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ot633bkwx8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Stores by Sales (with Concentration Metrics):\n",
      " Store # Total Sales % of Total Cumulative %\n",
      "      44 $62,087,553       5.8%         5.8%\n",
      "      45 $54,498,010       5.1%        10.9%\n",
      "      47 $50,948,310       4.7%        15.6%\n",
      "       3 $50,481,910       4.7%        20.3%\n",
      "      49 $43,420,096       4.0%        24.4%\n",
      "      46 $41,896,062       3.9%        28.3%\n",
      "      48 $35,933,130       3.3%        31.6%\n",
      "      51 $32,911,490       3.1%        34.7%\n",
      "       8 $30,494,287       2.8%        37.5%\n",
      "      50 $28,653,021       2.7%        40.2%\n",
      "\n",
      " Store Concentration Insights:\n",
      "- Top 4 stores (7.4% of locations) generate 20.3% of revenue\n",
      "- Top 10 stores (18.5% of locations) generate 40.2% of revenue\n",
      "- Highest performing store (#44) generates 23.0x more than lowest\n",
      "- This indicates EXTREME HETEROGENEITY across store locations\n"
     ]
    }
   ],
   "source": [
    "# Store-Level Concentration Metrics\n",
    "\n",
    "# Calculate cumulative percentages\n",
    "store_sales_sorted = sales_by_store.sort_values(ascending=False)\n",
    "store_cumsum = store_sales_sorted.cumsum()\n",
    "store_cumsum_pct = (store_cumsum / total_sales) * 100\n",
    "\n",
    "# Create top 10 stores with concentration metrics\n",
    "top_10_stores = pd.DataFrame({\n",
    "    'Store #': store_sales_sorted.head(10).index,\n",
    "    'Total Sales': [f'${x:,.0f}' for x in store_sales_sorted.head(10).values],\n",
    "    '% of Total': [f'{(x/total_sales)*100:.1f}%' for x in store_sales_sorted.head(10).values],\n",
    "    'Cumulative %': [f'{store_cumsum_pct.iloc[i]:.1f}%' for i in range(10)]\n",
    "})\n",
    "\n",
    "print(\"Top 10 Stores by Sales (with Concentration Metrics):\")\n",
    "print(top_10_stores.to_string(index=False))\n",
    "\n",
    "# Calculate concentration statistics\n",
    "top_4_stores_pct = (store_sales_sorted.head(4).sum() / total_sales) * 100\n",
    "top_10_stores_pct = (store_sales_sorted.head(10).sum() / total_sales) * 100\n",
    "total_stores = df_train['store_nbr'].nunique()\n",
    "\n",
    "# Store heterogeneity: max vs min\n",
    "max_store_sales = store_sales_sorted.iloc[0]\n",
    "min_store_sales = store_sales_sorted.iloc[-1]\n",
    "store_variance_ratio = max_store_sales / min_store_sales\n",
    "\n",
    "print(f\"\\n Store Concentration Insights:\")\n",
    "print(f\"- Top 4 stores (7.4% of locations) generate {top_4_stores_pct:.1f}% of revenue\")\n",
    "print(f\"- Top 10 stores ({10/total_stores*100:.1f}% of locations) generate {top_10_stores_pct:.1f}% of revenue\")\n",
    "print(f\"- Highest performing store (#{store_sales_sorted.index[0]}) generates {store_variance_ratio:.1f}x more than lowest\")\n",
    "print(f\"- This indicates EXTREME HETEROGENEITY across store locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k52qb7su79",
   "metadata": {},
   "source": [
    "### Product Family Concentration Analysis\n",
    "\n",
    "Analyzing which product families drive the most revenue and their contribution to total sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6v5yopx5rro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Product Families by Sales (with Concentration Metrics):\n",
      "Product Family  Total Sales % of Total Cumulative %\n",
      "     GROCERY I $343,462,735      32.0%        32.0%\n",
      "     BEVERAGES $216,954,486      20.2%        52.2%\n",
      "       PRODUCE $122,704,685      11.4%        63.6%\n",
      "      CLEANING  $97,521,289       9.1%        72.7%\n",
      "         DAIRY  $64,487,709       6.0%        78.7%\n",
      "  BREAD/BAKERY  $42,133,946       3.9%        82.6%\n",
      "       POULTRY  $31,876,004       3.0%        85.6%\n",
      "         MEATS  $31,086,468       2.9%        88.5%\n",
      " PERSONAL CARE  $24,592,051       2.3%        90.8%\n",
      "          DELI  $24,110,322       2.2%        93.0%\n",
      "\n",
      " Product Family Concentration Insights:\n",
      "- Top 3 families (GROCERY I, BEVERAGES, PRODUCE) account for 63.6% of total revenue\n",
      "- Top 5 families (15.2% of categories) generate 78.7% of revenue\n",
      "- Top 10 families (30.3% of categories) generate 93.0% of revenue\n",
      "- This shows HEAVY CONCENTRATION in daily necessities categories\n"
     ]
    }
   ],
   "source": [
    "# Product Family Concentration Metrics\n",
    "\n",
    "# Calculate cumulative percentages for families\n",
    "family_sales_sorted = sales_by_family.sort_values(ascending=False)\n",
    "family_cumsum = family_sales_sorted.cumsum()\n",
    "family_cumsum_pct = (family_cumsum / total_sales) * 100\n",
    "\n",
    "# Create top 10 families with concentration metrics\n",
    "top_10_families = pd.DataFrame({\n",
    "    'Product Family': family_sales_sorted.head(10).index,\n",
    "    'Total Sales': [f'${x:,.0f}' for x in family_sales_sorted.head(10).values],\n",
    "    '% of Total': [f'{(x/total_sales)*100:.1f}%' for x in family_sales_sorted.head(10).values],\n",
    "    'Cumulative %': [f'{family_cumsum_pct.iloc[i]:.1f}%' for i in range(10)]\n",
    "})\n",
    "\n",
    "print(\"Top 10 Product Families by Sales (with Concentration Metrics):\")\n",
    "print(top_10_families.to_string(index=False))\n",
    "\n",
    "# Calculate concentration statistics\n",
    "top_3_families_pct = (family_sales_sorted.head(3).sum() / total_sales) * 100\n",
    "top_5_families_pct = (family_sales_sorted.head(5).sum() / total_sales) * 100\n",
    "top_10_families_pct = (family_sales_sorted.head(10).sum() / total_sales) * 100\n",
    "total_families = df_train['family'].nunique()\n",
    "\n",
    "print(f\"\\n Product Family Concentration Insights:\")\n",
    "print(f\"- Top 3 families (GROCERY I, BEVERAGES, PRODUCE) account for {top_3_families_pct:.1f}% of total revenue\")\n",
    "print(f\"- Top 5 families ({5/total_families*100:.1f}% of categories) generate {top_5_families_pct:.1f}% of revenue\")\n",
    "print(f\"- Top 10 families ({10/total_families*100:.1f}% of categories) generate {top_10_families_pct:.1f}% of revenue\")\n",
    "print(f\"- This shows HEAVY CONCENTRATION in daily necessities categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6679bc",
   "metadata": {},
   "source": [
    "### Analyze 'onpromotion' column and calculate detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "501eb23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'onpromotion' occurrences: 7810622\n",
      "Percentage of rows with promotion: 260.28%\n",
      "Zero 'onpromotion' count: 2389559 (79.63%)\n",
      "\n",
      "Top 10 stores by promotion count:\n",
      " store_nbr\n",
      "53    204016\n",
      "47    192725\n",
      "44    192449\n",
      "45    191503\n",
      "46    190697\n",
      "48    185566\n",
      "49    184736\n",
      "9     177356\n",
      "3     177075\n",
      "50    174115\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Top 10 families by promotion count:\n",
      " family\n",
      "GROCERY I        1914801\n",
      "PRODUCE          1117921\n",
      "BEVERAGES         906958\n",
      "DAIRY             728707\n",
      "CLEANING          661157\n",
      "DELI              583316\n",
      "BREAD/BAKERY      331289\n",
      "MEATS             304028\n",
      "PERSONAL CARE     246928\n",
      "POULTRY           226421\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Daily promotion sample:\n",
      " date\n",
      "2013-01-01    0\n",
      "2013-01-02    0\n",
      "2013-01-03    0\n",
      "2013-01-04    0\n",
      "2013-01-05    0\n",
      "Name: onpromotion, dtype: int64\n",
      "\n",
      "Promotion summary:\n",
      "    total_onpromotion  pct_onpromotion  unique_promo_days\n",
      "0            7810622       260.277025               1230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Total promotion occurrences and percentage\n",
    "total_onpromo = df_train['onpromotion'].sum()\n",
    "pct_onpromo = total_onpromo / len(df_train) * 100\n",
    "zero_onpromo_count = (df_train['onpromotion'] == 0).sum()\n",
    "pct_zero_onpromo = zero_onpromo_count / len(df_train) * 100\n",
    "print(f\"Total 'onpromotion' occurrences: {total_onpromo}\")\n",
    "print(f\"Percentage of rows with promotion: {pct_onpromo:.2f}%\")\n",
    "print(f\"Zero 'onpromotion' count: {zero_onpromo_count} ({pct_zero_onpromo:.2f}%)\")\n",
    "\n",
    "# Promotions by store and family (counts and percent)\n",
    "onpromo_by_store = df_train.groupby('store_nbr')['onpromotion'].sum().sort_values(ascending=False)\n",
    "onpromo_by_family = df_train.groupby('family')['onpromotion'].sum().sort_values(ascending=False)\n",
    "onpromo_by_store_pct = (df_train.groupby('store_nbr')['onpromotion'].mean()*100).sort_values(ascending=False)\n",
    "onpromo_by_family_pct = (df_train.groupby('family')['onpromotion'].mean()*100).sort_values(ascending=False)\n",
    "\n",
    "print('\\nTop 10 stores by promotion count:\\n', onpromo_by_store.head(10))\n",
    "print('\\nTop 10 families by promotion count:\\n', onpromo_by_family.head(10))\n",
    "\n",
    "# Daily promotion counts (time series) - ensure 'date' is datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "daily_onpromo = df_train.groupby('date')['onpromotion'].sum()\n",
    "print('\\nDaily promotion sample:\\n', daily_onpromo.head())\n",
    "\n",
    "# Create a summary DataFrame\n",
    "promotion_summary = pd.DataFrame({\n",
    "    'total_onpromotion': [total_onpromo],\n",
    "    'pct_onpromotion': [pct_onpromo],\n",
    "    'unique_promo_days': [daily_onpromo[daily_onpromo>0].shape[0]]\n",
    "})\n",
    "print('\\nPromotion summary:\\n', promotion_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zozgvaxxkjj",
   "metadata": {},
   "source": [
    "### Promotional Strategy Analysis\n",
    "\n",
    "Analyzing how promotions are distributed across stores and product families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qc97ci7k8h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Stores by Promotional Activity:\n",
      " Store # Total Promo Items % of All Promos Cumulative %\n",
      "      53           204,016            2.6%         2.6%\n",
      "      47           192,725            2.5%         5.1%\n",
      "      44           192,449            2.5%         7.5%\n",
      "      45           191,503            2.5%        10.0%\n",
      "      46           190,697            2.4%        12.4%\n",
      "\n",
      "Top 5 Product Families by Promotional Activity:\n",
      "Product Family Total Promo Items % of All Promos Cumulative %\n",
      "     GROCERY I         1,914,801           24.5%        24.5%\n",
      "       PRODUCE         1,117,921           14.3%        38.8%\n",
      "     BEVERAGES           906,958           11.6%        50.4%\n",
      "         DAIRY           728,707            9.3%        59.8%\n",
      "      CLEANING           661,157            8.5%        68.2%\n",
      "\n",
      " Promotional Strategy Insights:\n",
      "- 79.6% of records have NO promotions, while 20.4% have at least one\n",
      "- Promotions active on 1230 out of 1684 days (73.0%)\n",
      "- Top 5 families receive 68.2% of all promotional activity\n",
      "- Promotions are CATEGORY-SPECIFIC, targeting high-revenue families\n"
     ]
    }
   ],
   "source": [
    "# Promotional Concentration Analysis\n",
    "\n",
    "# Top stores by promotional activity\n",
    "promo_sorted_stores = onpromo_by_store.sort_values(ascending=False)\n",
    "promo_cumsum_stores = promo_sorted_stores.cumsum()\n",
    "promo_cumsum_pct_stores = (promo_cumsum_stores / total_onpromo) * 100\n",
    "\n",
    "top_5_promo_stores = pd.DataFrame({\n",
    "    'Store #': promo_sorted_stores.head(5).index,\n",
    "    'Total Promo Items': [f'{x:,}' for x in promo_sorted_stores.head(5).values],\n",
    "    '% of All Promos': [f'{(x/total_onpromo)*100:.1f}%' for x in promo_sorted_stores.head(5).values],\n",
    "    'Cumulative %': [f'{promo_cumsum_pct_stores.iloc[i]:.1f}%' for i in range(5)]\n",
    "})\n",
    "\n",
    "print(\"Top 5 Stores by Promotional Activity:\")\n",
    "print(top_5_promo_stores.to_string(index=False))\n",
    "\n",
    "# Top families by promotional activity  \n",
    "promo_sorted_families = onpromo_by_family.sort_values(ascending=False)\n",
    "promo_cumsum_families = promo_sorted_families.cumsum()\n",
    "promo_cumsum_pct_families = (promo_cumsum_families / total_onpromo) * 100\n",
    "\n",
    "top_5_promo_families = pd.DataFrame({\n",
    "    'Product Family': promo_sorted_families.head(5).index,\n",
    "    'Total Promo Items': [f'{x:,}' for x in promo_sorted_families.head(5).values],\n",
    "    '% of All Promos': [f'{(x/total_onpromo)*100:.1f}%' for x in promo_sorted_families.head(5).values],\n",
    "    'Cumulative %': [f'{promo_cumsum_pct_families.iloc[i]:.1f}%' for i in range(5)]\n",
    "})\n",
    "\n",
    "print(\"\\nTop 5 Product Families by Promotional Activity:\")\n",
    "print(top_5_promo_families.to_string(index=False))\n",
    "\n",
    "# Promotional insights\n",
    "top_5_promo_families_pct = (promo_sorted_families.head(5).sum() / total_onpromo) * 100\n",
    "days_with_promos = (daily_onpromo > 0).sum()\n",
    "total_days = daily_onpromo.shape[0]\n",
    "days_with_promos_pct = (days_with_promos / total_days) * 100\n",
    "\n",
    "print(f\"\\n Promotional Strategy Insights:\")\n",
    "print(f\"- {pct_zero_onpromo:.1f}% of records have NO promotions, while {100-pct_zero_onpromo:.1f}% have at least one\")\n",
    "print(f\"- Promotions active on {days_with_promos} out of {total_days} days ({days_with_promos_pct:.1f}%)\")\n",
    "print(f\"- Top 5 families receive {top_5_promo_families_pct:.1f}% of all promotional activity\")\n",
    "print(f\"- Promotions are CATEGORY-SPECIFIC, targeting high-revenue families\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
