{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3efa301",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Analyzing hotel booking data to understand customer behavior and cancellation trends."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 6729f334aec1ce53652a4a2114af4867caeede1a
   "id": "2bccb21f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Topic_13_Project\\\\Topic_13_Retail_Store_Sales_Time_Series\\\\data\\\\raw\\\\transactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTopic_13_Project\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTopic_13_Retail_Store_Sales_Time_Series\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtransactions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Topic_13_Project\\\\Topic_13_Retail_Store_Sales_Time_Series\\\\data\\\\raw\\\\transactions.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"D:\\Topic_13_Project\\Topic_13_Retail_Store_Sales_Time_Series\\data\\raw\\transactions.csv\")\n",
    "\n",
    "df.head(5)\n",
    "df.info()\n",
    "df.describe()\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b4fb0",
   "metadata": {},
   "source": [
    "## Cleaning steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce87d81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     53\u001b[0m     panel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m panel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m panel\n\u001b[1;32m---> 58\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m clean_transactions(\u001b[43mdf\u001b[49m)\n\u001b[0;32m     59\u001b[0m df_clean\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_transactions(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "\n",
    "    # 1. Standardize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required_cols = {\"date\", \"store_nbr\", \"transactions\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # 2. Parse date\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    # 3. Coerce numeric types\n",
    "    df[\"store_nbr\"] = pd.to_numeric(df[\"store_nbr\"], errors=\"coerce\")\n",
    "    df[\"transactions\"] = pd.to_numeric(df[\"transactions\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid store/transactions\n",
    "    df = df.dropna(subset=[\"store_nbr\", \"transactions\"])\n",
    "\n",
    "    # store_nbr must be integer\n",
    "    df[\"store_nbr\"] = df[\"store_nbr\"].astype(int)\n",
    "\n",
    "    # 4. Handle invalid transactions (should be non-negative counts)\n",
    "    df.loc[df[\"transactions\"] < 0, \"transactions\"] = np.nan\n",
    "    df = df.dropna(subset=[\"transactions\"])\n",
    "    df[\"transactions\"] = df[\"transactions\"].astype(int)\n",
    "\n",
    "    # 5. Resolve duplicates: sum transactions per (date, store_nbr)\n",
    "    df = (\n",
    "        df.groupby([\"date\", \"store_nbr\"], as_index=False)[\"transactions\"]\n",
    "          .sum()\n",
    "          .sort_values([\"date\", \"store_nbr\"])\n",
    "    )\n",
    "\n",
    "    # 6. Build complete panel (all dates x all stores)\n",
    "    all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"D\")\n",
    "    all_stores = np.sort(df[\"store_nbr\"].unique())\n",
    "\n",
    "    panel = (\n",
    "        pd.MultiIndex.from_product([all_dates, all_stores], names=[\"date\", \"store_nbr\"])\n",
    "          .to_frame(index=False)\n",
    "          .merge(df, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # 7. Fill missing with 0 and flag imputed\n",
    "    panel[\"is_imputed\"] = panel[\"transactions\"].isna().astype(int)\n",
    "    panel[\"transactions\"] = panel[\"transactions\"].fillna(0).astype(int)\n",
    "\n",
    "    # Make date string for clean CSV (easy to submit/read)\n",
    "    panel[\"date\"] = panel[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return panel\n",
    "\n",
    "\n",
    "df_clean = clean_transactions(df)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec92b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== QUICK CHECKS ======\n",
    "print(\"Shape:\", df_clean.shape)\n",
    "print(\"Missing values:\\n\", df_clean.isnull().sum())\n",
    "print(\"Duplicate (date, store_nbr):\", df_clean.duplicated(subset=[\"date\", \"store_nbr\"]).sum())\n",
    "print(\"Unique stores:\", df_clean[\"store_nbr\"].nunique())\n",
    "print(\"Unique days:\", pd.to_datetime(df_clean[\"date\"]).nunique())\n",
    "\n",
    "df_clean.describe(include=\"all\").T.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6lm3vpili",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "## Transaction Volume Distribution Analysis\n",
    "\n",
    "Analyzing the statistical characteristics of transaction counts across stores and time."
   ]
=======
   "source": "## Transaction Volume Distribution Analysis\n\nAnalyzing the statistical characteristics of transaction counts across stores and time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wsqf2ljwi6k",
   "source": "# Transaction Volume Statistics (excluding imputed zeros)\nfrom scipy import stats\n\ndf_actual = df_clean[df_clean['is_imputed'] == 0].copy()\n\n# Basic statistics\nmean_trans = df_actual['transactions'].mean()\nmedian_trans = df_actual['transactions'].median()\nstd_trans = df_actual['transactions'].std()\nmin_trans = df_actual['transactions'].min()\nmax_trans = df_actual['transactions'].max()\nq25_trans = df_actual['transactions'].quantile(0.25)\nq75_trans = df_actual['transactions'].quantile(0.75)\n\n# Distribution shape metrics\nskewness_trans = stats.skew(df_actual['transactions'])\ncv_trans = (std_trans / mean_trans) * 100\n\nprint(\"Transaction Volume Distribution (Actual Data Only):\")\nprint(\"=\" * 60)\nprint(f\"Mean transactions/day: {mean_trans:,.0f}\")\nprint(f\"Median transactions/day: {median_trans:,.0f}\")\nprint(f\"Std deviation: {std_trans:,.0f}\")\nprint(f\"Min transactions: {min_trans:,}\")\nprint(f\"Max transactions: {max_trans:,}\")\nprint(f\"25th percentile: {q25_trans:,.0f}\")\nprint(f\"75th percentile: {q75_trans:,.0f}\")\nprint(f\"\\nDistribution Characteristics:\")\nprint(f\"Coefficient of Variation: {cv_trans:.1f}%\")\nprint(f\"Skewness: {skewness_trans:.2f}\")\nprint(f\"Max/Mean ratio: {max_trans/mean_trans:.1f}x\")\n\nprint(f\"\\nüìä Interpretation:\")\nprint(f\"- High CV ({cv_trans:.1f}%) indicates SIGNIFICANT VARIABILITY across stores\")\nprint(f\"- Positive skewness ({skewness_trans:.2f}) shows right-tailed distribution\")\nprint(f\"- Busiest store has {max_trans/mean_trans:.1f}x more transactions than average\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bbsgkyc8tp",
   "source": "## Store-Level Transaction Heterogeneity\n\nAnalyzing how transaction volumes vary across different stores to understand traffic patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mktf32glbih",
   "source": "# Store-level transaction analysis\nstore_trans = df_actual.groupby('store_nbr')['transactions'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\nstore_trans = store_trans.sort_values('mean', ascending=False)\n\n# Top and bottom performers\ntop_5_stores = store_trans.head(5)\nbottom_5_stores = store_trans.tail(5)\n\nprint(\"Top 5 Stores by Average Daily Transactions:\")\nprint(\"=\" * 60)\nfor idx, row in top_5_stores.iterrows():\n    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n\nprint(f\"\\nBottom 5 Stores by Average Daily Transactions:\")\nprint(\"=\" * 60)\nfor idx, row in bottom_5_stores.iterrows():\n    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n\n# Heterogeneity metrics\nbusiest_store = store_trans.index[0]\nquietest_store = store_trans.index[-1]\nheterogeneity_ratio = store_trans['mean'].iloc[0] / store_trans['mean'].iloc[-1]\n\nprint(f\"\\nüè™ Store Heterogeneity Insights:\")\nprint(f\"- Busiest store (#{busiest_store}): {store_trans['mean'].iloc[0]:,.0f} avg transactions/day\")\nprint(f\"- Quietest store (#{quietest_store}): {store_trans['mean'].iloc[-1]:,.0f} avg transactions/day\")\nprint(f\"- Heterogeneity ratio: {heterogeneity_ratio:.1f}x difference between busiest and quietest\")\nprint(f\"- This indicates EXTREME STORE-LEVEL VARIATION in foot traffic\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "te9lztk0yql",
   "source": "## Weekly Seasonality Analysis\n\nAnalyzing day-of-week patterns to identify weekend and weekday traffic differences.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mmbmu9bz8zl",
   "source": "# Weekly pattern analysis\ndf_actual['date_dt'] = pd.to_datetime(df_actual['date'])\ndf_actual['day_of_week'] = df_actual['date_dt'].dt.day_name()\ndf_actual['is_weekend'] = df_actual['date_dt'].dt.dayofweek.isin([5, 6]).astype(int)\n\n# Average transactions by day of week\ndow_trans = df_actual.groupby('day_of_week')['transactions'].mean().reindex([\n    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n])\n\n# Calculate weekend uplift\nweekday_avg = df_actual[df_actual['is_weekend'] == 0]['transactions'].mean()\nweekend_avg = df_actual[df_actual['is_weekend'] == 1]['transactions'].mean()\nweekend_uplift = ((weekend_avg - weekday_avg) / weekday_avg) * 100\n\nprint(\"Average Transactions by Day of Week:\")\nprint(\"=\" * 60)\nfor day, avg_trans in dow_trans.items():\n    vs_weekday = ((avg_trans - weekday_avg) / weekday_avg) * 100\n    print(f\"{day:12s}: {avg_trans:,.0f} ({vs_weekday:+.1f}% vs weekday avg)\")\n\nprint(f\"\\nWeekday vs Weekend Comparison:\")\nprint(\"=\" * 60)\nprint(f\"Weekday average: {weekday_avg:,.0f} transactions\")\nprint(f\"Weekend average: {weekend_avg:,.0f} transactions\")\nprint(f\"Weekend uplift: {weekend_uplift:+.1f}%\")\n\n# Find peak day\npeak_day = dow_trans.idxmax()\npeak_value = dow_trans.max()\n\nprint(f\"\\nüìÖ Weekly Seasonality Insights:\")\nprint(f\"- Peak day: {peak_day} with {peak_value:,.0f} avg transactions\")\nprint(f\"- Weekend shows {weekend_uplift:+.1f}% traffic boost vs weekdays\")\nprint(f\"- Strong WEEKLY SEASONALITY pattern detected\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
>>>>>>> 6729f334aec1ce53652a4a2114af4867caeede1a
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsqf2ljwi6k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Volume Statistics (excluding imputed zeros)\n",
    "from scipy import stats\n",
    "\n",
    "df_actual = df_clean[df_clean['is_imputed'] == 0].copy()\n",
    "\n",
    "# Basic statistics\n",
    "mean_trans = df_actual['transactions'].mean()\n",
    "median_trans = df_actual['transactions'].median()\n",
    "std_trans = df_actual['transactions'].std()\n",
    "min_trans = df_actual['transactions'].min()\n",
    "max_trans = df_actual['transactions'].max()\n",
    "q25_trans = df_actual['transactions'].quantile(0.25)\n",
    "q75_trans = df_actual['transactions'].quantile(0.75)\n",
    "\n",
    "# Distribution shape metrics\n",
    "skewness_trans = stats.skew(df_actual['transactions'])\n",
    "cv_trans = (std_trans / mean_trans) * 100\n",
    "\n",
    "print(\"Transaction Volume Distribution (Actual Data Only):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean transactions/day: {mean_trans:,.0f}\")\n",
    "print(f\"Median transactions/day: {median_trans:,.0f}\")\n",
    "print(f\"Std deviation: {std_trans:,.0f}\")\n",
    "print(f\"Min transactions: {min_trans:,}\")\n",
    "print(f\"Max transactions: {max_trans:,}\")\n",
    "print(f\"25th percentile: {q25_trans:,.0f}\")\n",
    "print(f\"75th percentile: {q75_trans:,.0f}\")\n",
    "print(f\"\\nDistribution Characteristics:\")\n",
    "print(f\"Coefficient of Variation: {cv_trans:.1f}%\")\n",
    "print(f\"Skewness: {skewness_trans:.2f}\")\n",
    "print(f\"Max/Mean ratio: {max_trans/mean_trans:.1f}x\")\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"- High CV ({cv_trans:.1f}%) indicates SIGNIFICANT VARIABILITY across stores\")\n",
    "print(f\"- Positive skewness ({skewness_trans:.2f}) shows right-tailed distribution\")\n",
    "print(f\"- Busiest store has {max_trans/mean_trans:.1f}x more transactions than average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbsgkyc8tp",
   "metadata": {},
   "source": [
    "## Store-Level Transaction Heterogeneity\n",
    "\n",
    "Analyzing how transaction volumes vary across different stores to understand traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mktf32glbih",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store-level transaction analysis\n",
    "store_trans = df_actual.groupby('store_nbr')['transactions'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "store_trans = store_trans.sort_values('mean', ascending=False)\n",
    "\n",
    "# Top and bottom performers\n",
    "top_5_stores = store_trans.head(5)\n",
    "bottom_5_stores = store_trans.tail(5)\n",
    "\n",
    "print(\"Top 5 Stores by Average Daily Transactions:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in top_5_stores.iterrows():\n",
    "    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n",
    "\n",
    "print(f\"\\nBottom 5 Stores by Average Daily Transactions:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in bottom_5_stores.iterrows():\n",
    "    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n",
    "\n",
    "# Heterogeneity metrics\n",
    "busiest_store = store_trans.index[0]\n",
    "quietest_store = store_trans.index[-1]\n",
    "heterogeneity_ratio = store_trans['mean'].iloc[0] / store_trans['mean'].iloc[-1]\n",
    "\n",
    "print(f\"\\nüè™ Store Heterogeneity Insights:\")\n",
    "print(f\"- Busiest store (#{busiest_store}): {store_trans['mean'].iloc[0]:,.0f} avg transactions/day\")\n",
    "print(f\"- Quietest store (#{quietest_store}): {store_trans['mean'].iloc[-1]:,.0f} avg transactions/day\")\n",
    "print(f\"- Heterogeneity ratio: {heterogeneity_ratio:.1f}x difference between busiest and quietest\")\n",
    "print(f\"- This indicates EXTREME STORE-LEVEL VARIATION in foot traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "te9lztk0yql",
   "metadata": {},
   "source": [
    "## Weekly Seasonality Analysis\n",
    "\n",
    "Analyzing day-of-week patterns to identify weekend and weekday traffic differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmbmu9bz8zl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly pattern analysis\n",
    "df_actual['date_dt'] = pd.to_datetime(df_actual['date'])\n",
    "df_actual['day_of_week'] = df_actual['date_dt'].dt.day_name()\n",
    "df_actual['is_weekend'] = df_actual['date_dt'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "# Average transactions by day of week\n",
    "dow_trans = df_actual.groupby('day_of_week')['transactions'].mean().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "])\n",
    "\n",
    "# Calculate weekend uplift\n",
    "weekday_avg = df_actual[df_actual['is_weekend'] == 0]['transactions'].mean()\n",
    "weekend_avg = df_actual[df_actual['is_weekend'] == 1]['transactions'].mean()\n",
    "weekend_uplift = ((weekend_avg - weekday_avg) / weekday_avg) * 100\n",
    "\n",
    "print(\"Average Transactions by Day of Week:\")\n",
    "print(\"=\" * 60)\n",
    "for day, avg_trans in dow_trans.items():\n",
    "    vs_weekday = ((avg_trans - weekday_avg) / weekday_avg) * 100\n",
    "    print(f\"{day:12s}: {avg_trans:,.0f} ({vs_weekday:+.1f}% vs weekday avg)\")\n",
    "\n",
    "print(f\"\\nWeekday vs Weekend Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Weekday average: {weekday_avg:,.0f} transactions\")\n",
    "print(f\"Weekend average: {weekend_avg:,.0f} transactions\")\n",
    "print(f\"Weekend uplift: {weekend_uplift:+.1f}%\")\n",
    "\n",
    "# Find peak day\n",
    "peak_day = dow_trans.idxmax()\n",
    "peak_value = dow_trans.max()\n",
    "\n",
    "print(f\"\\nüìÖ Weekly Seasonality Insights:\")\n",
    "print(f\"- Peak day: {peak_day} with {peak_value:,.0f} avg transactions\")\n",
    "print(f\"- Weekend shows {weekend_uplift:+.1f}% traffic boost vs weekdays\")\n",
    "print(f\"- Strong WEEKLY SEASONALITY pattern detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== EXPORT CLEANED DATA ======\n",
    "output_path = \"transactions_cleaned.csv\"\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "output_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}