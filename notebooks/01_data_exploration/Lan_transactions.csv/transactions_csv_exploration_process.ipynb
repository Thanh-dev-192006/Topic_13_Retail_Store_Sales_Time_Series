{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3efa301",
   "metadata": {},
   "source": [
    "# ðŸ“Š Exploratory Data Analysis (EDA)\n",
    "Analyzing hotel booking data to understand customer behavior and cancellation trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bccb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83488 entries, 0 to 83487\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          83488 non-null  object\n",
      " 1   store_nbr     83488 non-null  int64 \n",
      " 2   transactions  83488 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date            0\n",
       "store_nbr       0\n",
       "transactions    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"D:\\Topic_13_Project\\Topic_13_Retail_Store_Sales_Time_Series\\data\\raw\\transactions.csv\")\n",
    "\n",
    "df.head(5)\n",
    "df.info()\n",
    "df.describe()\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b4fb0",
   "metadata": {},
   "source": [
    "## Cleaning steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce87d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>transactions</th>\n",
       "      <th>is_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr  transactions  is_imputed\n",
       "0  2013-01-01          1             0           1\n",
       "1  2013-01-01          2             0           1\n",
       "2  2013-01-01          3             0           1\n",
       "3  2013-01-01          4             0           1\n",
       "4  2013-01-01          5             0           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_transactions(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "\n",
    "    # 1. Standardize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required_cols = {\"date\", \"store_nbr\", \"transactions\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # 2. Parse date\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    # 3. Coerce numeric types\n",
    "    df[\"store_nbr\"] = pd.to_numeric(df[\"store_nbr\"], errors=\"coerce\")\n",
    "    df[\"transactions\"] = pd.to_numeric(df[\"transactions\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with invalid store/transactions\n",
    "    df = df.dropna(subset=[\"store_nbr\", \"transactions\"])\n",
    "\n",
    "    # store_nbr must be integer\n",
    "    df[\"store_nbr\"] = df[\"store_nbr\"].astype(int)\n",
    "\n",
    "    # 4. Handle invalid transactions (should be non-negative counts)\n",
    "    df.loc[df[\"transactions\"] < 0, \"transactions\"] = np.nan\n",
    "    df = df.dropna(subset=[\"transactions\"])\n",
    "    df[\"transactions\"] = df[\"transactions\"].astype(int)\n",
    "\n",
    "    # 5. Resolve duplicates: sum transactions per (date, store_nbr)\n",
    "    df = (\n",
    "        df.groupby([\"date\", \"store_nbr\"], as_index=False)[\"transactions\"]\n",
    "          .sum()\n",
    "          .sort_values([\"date\", \"store_nbr\"])\n",
    "    )\n",
    "\n",
    "    # 6. Build complete panel (all dates x all stores)\n",
    "    all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=\"D\")\n",
    "    all_stores = np.sort(df[\"store_nbr\"].unique())\n",
    "\n",
    "    panel = (\n",
    "        pd.MultiIndex.from_product([all_dates, all_stores], names=[\"date\", \"store_nbr\"])\n",
    "          .to_frame(index=False)\n",
    "          .merge(df, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # 7. Fill missing with 0 and flag imputed\n",
    "    panel[\"is_imputed\"] = panel[\"transactions\"].isna().astype(int)\n",
    "    panel[\"transactions\"] = panel[\"transactions\"].fillna(0).astype(int)\n",
    "\n",
    "    # Make date string for clean CSV (easy to submit/read)\n",
    "    panel[\"date\"] = panel[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return panel\n",
    "\n",
    "\n",
    "df_clean = clean_transactions(df)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec92b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (91152, 4)\n",
      "Missing values:\n",
      " date            0\n",
      "store_nbr       0\n",
      "transactions    0\n",
      "is_imputed      0\n",
      "dtype: int64\n",
      "Duplicate (date, store_nbr): 0\n",
      "Unique stores: 54\n",
      "Unique days: 1688\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>91152</td>\n",
       "      <td>1688</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <td>91152.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.5</td>\n",
       "      <td>15.58587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transactions</th>\n",
       "      <td>91152.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1552.121127</td>\n",
       "      <td>1034.916203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>8359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_imputed</th>\n",
       "      <td>91152.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084079</td>\n",
       "      <td>0.277508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count unique         top freq         mean          std  min  \\\n",
       "date            91152   1688  2017-08-15   54          NaN          NaN  NaN   \n",
       "store_nbr     91152.0    NaN         NaN  NaN         27.5     15.58587  1.0   \n",
       "transactions  91152.0    NaN         NaN  NaN  1552.121127  1034.916203  0.0   \n",
       "is_imputed    91152.0    NaN         NaN  NaN     0.084079     0.277508  0.0   \n",
       "\n",
       "                25%     50%     75%     max  \n",
       "date            NaN     NaN     NaN     NaN  \n",
       "store_nbr      14.0    27.5    41.0    54.0  \n",
       "transactions  926.0  1329.0  1974.0  8359.0  \n",
       "is_imputed      0.0     0.0     0.0     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== QUICK CHECKS ======\n",
    "print(\"Shape:\", df_clean.shape)\n",
    "print(\"Missing values:\\n\", df_clean.isnull().sum())\n",
    "print(\"Duplicate (date, store_nbr):\", df_clean.duplicated(subset=[\"date\", \"store_nbr\"]).sum())\n",
    "print(\"Unique stores:\", df_clean[\"store_nbr\"].nunique())\n",
    "print(\"Unique days:\", pd.to_datetime(df_clean[\"date\"]).nunique())\n",
    "\n",
    "df_clean.describe(include=\"all\").T.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6lm3vpili",
   "metadata": {},
   "source": [
    "## Transaction Volume Distribution Analysis\n",
    "\n",
    "Analyzing the statistical characteristics of transaction counts across stores and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wsqf2ljwi6k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Volume Distribution (Actual Data Only):\n",
      "============================================================\n",
      "Mean transactions/day: 1,695\n",
      "Median transactions/day: 1,393\n",
      "Std deviation: 963\n",
      "Min transactions: 5\n",
      "Max transactions: 8,359\n",
      "25th percentile: 1,046\n",
      "75th percentile: 2,079\n",
      "\n",
      "Distribution Characteristics:\n",
      "Coefficient of Variation: 56.8%\n",
      "Skewness: 1.52\n",
      "Max/Mean ratio: 4.9x\n",
      "\n",
      " Interpretation:\n",
      "- High CV (56.8%) indicates SIGNIFICANT VARIABILITY across stores\n",
      "- Positive skewness (1.52) shows right-tailed distribution\n",
      "- Busiest store has 4.9x more transactions than average\n"
     ]
    }
   ],
   "source": [
    "# Transaction Volume Statistics (excluding imputed zeros)\n",
    "from scipy import stats\n",
    "\n",
    "df_actual = df_clean[df_clean['is_imputed'] == 0].copy()\n",
    "\n",
    "# Basic statistics\n",
    "mean_trans = df_actual['transactions'].mean()\n",
    "median_trans = df_actual['transactions'].median()\n",
    "std_trans = df_actual['transactions'].std()\n",
    "min_trans = df_actual['transactions'].min()\n",
    "max_trans = df_actual['transactions'].max()\n",
    "q25_trans = df_actual['transactions'].quantile(0.25)\n",
    "q75_trans = df_actual['transactions'].quantile(0.75)\n",
    "\n",
    "# Distribution shape metrics\n",
    "skewness_trans = stats.skew(df_actual['transactions'])\n",
    "cv_trans = (std_trans / mean_trans) * 100\n",
    "\n",
    "print(\"Transaction Volume Distribution (Actual Data Only):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean transactions/day: {mean_trans:,.0f}\")\n",
    "print(f\"Median transactions/day: {median_trans:,.0f}\")\n",
    "print(f\"Std deviation: {std_trans:,.0f}\")\n",
    "print(f\"Min transactions: {min_trans:,}\")\n",
    "print(f\"Max transactions: {max_trans:,}\")\n",
    "print(f\"25th percentile: {q25_trans:,.0f}\")\n",
    "print(f\"75th percentile: {q75_trans:,.0f}\")\n",
    "print(f\"\\nDistribution Characteristics:\")\n",
    "print(f\"Coefficient of Variation: {cv_trans:.1f}%\")\n",
    "print(f\"Skewness: {skewness_trans:.2f}\")\n",
    "print(f\"Max/Mean ratio: {max_trans/mean_trans:.1f}x\")\n",
    "\n",
    "print(f\"\\n Interpretation:\")\n",
    "print(f\"- High CV ({cv_trans:.1f}%) indicates SIGNIFICANT VARIABILITY across stores\")\n",
    "print(f\"- Positive skewness ({skewness_trans:.2f}) shows right-tailed distribution\")\n",
    "print(f\"- Busiest store has {max_trans/mean_trans:.1f}x more transactions than average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbsgkyc8tp",
   "metadata": {},
   "source": [
    "## Store-Level Transaction Heterogeneity\n",
    "\n",
    "Analyzing how transaction volumes vary across different stores to understand traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mktf32glbih",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Stores by Average Daily Transactions:\n",
      "============================================================\n",
      "Store #44: Avg=4,337, Median=4,170, Max=8,359\n",
      "Store #47: Avg=3,897, Median=3,685, Max=7,727\n",
      "Store #45: Avg=3,698, Median=3,515, Max=7,305\n",
      "Store #46: Avg=3,572, Median=3,230, Max=8,001\n",
      "Store #3: Avg=3,202, Median=3,100, Max=6,085\n",
      "\n",
      "Bottom 5 Stores by Average Daily Transactions:\n",
      "============================================================\n",
      "Store #22: Avg=751, Median=730, Max=2,412\n",
      "Store #30: Avg=708, Median=701, Max=1,443\n",
      "Store #35: Avg=671, Median=656, Max=1,676\n",
      "Store #32: Avg=635, Median=615, Max=1,497\n",
      "Store #26: Avg=635, Median=591, Max=2,184\n",
      "\n",
      " Store Heterogeneity Insights:\n",
      "- Busiest store (#44): 4,337 avg transactions/day\n",
      "- Quietest store (#26): 635 avg transactions/day\n",
      "- Heterogeneity ratio: 6.8x difference between busiest and quietest\n",
      "- This indicates EXTREME STORE-LEVEL VARIATION in foot traffic\n"
     ]
    }
   ],
   "source": [
    "# Store-level transaction analysis\n",
    "store_trans = df_actual.groupby('store_nbr')['transactions'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "store_trans = store_trans.sort_values('mean', ascending=False)\n",
    "\n",
    "# Top and bottom performers\n",
    "top_5_stores = store_trans.head(5)\n",
    "bottom_5_stores = store_trans.tail(5)\n",
    "\n",
    "print(\"Top 5 Stores by Average Daily Transactions:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in top_5_stores.iterrows():\n",
    "    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n",
    "\n",
    "print(f\"\\nBottom 5 Stores by Average Daily Transactions:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in bottom_5_stores.iterrows():\n",
    "    print(f\"Store #{idx}: Avg={row['mean']:,.0f}, Median={row['median']:,.0f}, Max={row['max']:,.0f}\")\n",
    "\n",
    "# Heterogeneity metrics\n",
    "busiest_store = store_trans.index[0]\n",
    "quietest_store = store_trans.index[-1]\n",
    "heterogeneity_ratio = store_trans['mean'].iloc[0] / store_trans['mean'].iloc[-1]\n",
    "\n",
    "print(f\"\\n Store Heterogeneity Insights:\")\n",
    "print(f\"- Busiest store (#{busiest_store}): {store_trans['mean'].iloc[0]:,.0f} avg transactions/day\")\n",
    "print(f\"- Quietest store (#{quietest_store}): {store_trans['mean'].iloc[-1]:,.0f} avg transactions/day\")\n",
    "print(f\"- Heterogeneity ratio: {heterogeneity_ratio:.1f}x difference between busiest and quietest\")\n",
    "print(f\"- This indicates EXTREME STORE-LEVEL VARIATION in foot traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "te9lztk0yql",
   "metadata": {},
   "source": [
    "## Weekly Seasonality Analysis\n",
    "\n",
    "Analyzing day-of-week patterns to identify weekend and weekday traffic differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mmbmu9bz8zl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Transactions by Day of Week:\n",
      "============================================================\n",
      "Monday      : 1,636 (+1.4% vs weekday avg)\n",
      "Tuesday     : 1,601 (-0.7% vs weekday avg)\n",
      "Wednesday   : 1,621 (+0.6% vs weekday avg)\n",
      "Thursday    : 1,550 (-3.9% vs weekday avg)\n",
      "Friday      : 1,654 (+2.6% vs weekday avg)\n",
      "Saturday    : 1,953 (+21.1% vs weekday avg)\n",
      "Sunday      : 1,847 (+14.6% vs weekday avg)\n",
      "\n",
      "Weekday vs Weekend Comparison:\n",
      "============================================================\n",
      "Weekday average: 1,612 transactions\n",
      "Weekend average: 1,900 transactions\n",
      "Weekend uplift: +17.9%\n",
      "\n",
      " Weekly Seasonality Insights:\n",
      "- Peak day: Saturday with 1,953 avg transactions\n",
      "- Weekend shows +17.9% traffic boost vs weekdays\n",
      "- Strong WEEKLY SEASONALITY pattern detected\n"
     ]
    }
   ],
   "source": [
    "# Weekly pattern analysis\n",
    "df_actual['date_dt'] = pd.to_datetime(df_actual['date'])\n",
    "df_actual['day_of_week'] = df_actual['date_dt'].dt.day_name()\n",
    "df_actual['is_weekend'] = df_actual['date_dt'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "# Average transactions by day of week\n",
    "dow_trans = df_actual.groupby('day_of_week')['transactions'].mean().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "])\n",
    "\n",
    "# Calculate weekend uplift\n",
    "weekday_avg = df_actual[df_actual['is_weekend'] == 0]['transactions'].mean()\n",
    "weekend_avg = df_actual[df_actual['is_weekend'] == 1]['transactions'].mean()\n",
    "weekend_uplift = ((weekend_avg - weekday_avg) / weekday_avg) * 100\n",
    "\n",
    "print(\"Average Transactions by Day of Week:\")\n",
    "print(\"=\" * 60)\n",
    "for day, avg_trans in dow_trans.items():\n",
    "    vs_weekday = ((avg_trans - weekday_avg) / weekday_avg) * 100\n",
    "    print(f\"{day:12s}: {avg_trans:,.0f} ({vs_weekday:+.1f}% vs weekday avg)\")\n",
    "\n",
    "print(f\"\\nWeekday vs Weekend Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Weekday average: {weekday_avg:,.0f} transactions\")\n",
    "print(f\"Weekend average: {weekend_avg:,.0f} transactions\")\n",
    "print(f\"Weekend uplift: {weekend_uplift:+.1f}%\")\n",
    "\n",
    "# Find peak day\n",
    "peak_day = dow_trans.idxmax()\n",
    "peak_value = dow_trans.max()\n",
    "\n",
    "print(f\"\\n Weekly Seasonality Insights:\")\n",
    "print(f\"- Peak day: {peak_day} with {peak_value:,.0f} avg transactions\")\n",
    "print(f\"- Weekend shows {weekend_uplift:+.1f}% traffic boost vs weekdays\")\n",
    "print(f\"- Strong WEEKLY SEASONALITY pattern detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transactions_cleaned.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== EXPORT CLEANED DATA ======\n",
    "output_path = \"transactions_cleaned.csv\"\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "output_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
